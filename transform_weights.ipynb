{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the ModelGenesis weights from Keras to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import keras\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Need to clone https://github.com/MrGiovanni/ModelsGenesis in advance\n",
    "from ModelsGenesis.keras.unet3d import *\n",
    "from src.model.nets import ModelsGenesisSegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_weight_path = './ModelsGenesis/pretrained_weights/Genesis_Chest_CT.h5' # ModelGenesis pretrained weights\n",
    "pytorch_weight_path = './weights/models_genesis.pth' # path to save the transformed Pytorch weights\n",
    "\n",
    "def forward_hook(self, inputs, outputs):\n",
    "    features_hook.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1, 64, 64, 32 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "depth_0_conv (Conv3D)           (None, 32, 64, 64, 3 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "depth_0_bn (BatchNormalization) (None, 32, 64, 64, 3 128         depth_0_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_0_relu (Activation)       (None, 32, 64, 64, 3 0           depth_0_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "depth_1_conv (Conv3D)           (None, 64, 64, 64, 3 55360       depth_0_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_1_bn (BatchNormalization) (None, 64, 64, 64, 3 256         depth_1_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_1_relu (Activation)       (None, 64, 64, 64, 3 0           depth_1_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, 64, 32, 32, 1 0           depth_1_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_2_conv (Conv3D)           (None, 64, 32, 32, 1 110656      max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depth_2_bn (BatchNormalization) (None, 64, 32, 32, 1 256         depth_2_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_2_relu (Activation)       (None, 64, 32, 32, 1 0           depth_2_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "depth_3_conv (Conv3D)           (None, 128, 32, 32,  221312      depth_2_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_3_bn (BatchNormalization) (None, 128, 32, 32,  512         depth_3_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_3_relu (Activation)       (None, 128, 32, 32,  0           depth_3_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, 128, 16, 16,  0           depth_3_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_4_conv (Conv3D)           (None, 128, 16, 16,  442496      max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depth_4_bn (BatchNormalization) (None, 128, 16, 16,  512         depth_4_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_4_relu (Activation)       (None, 128, 16, 16,  0           depth_4_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "depth_5_conv (Conv3D)           (None, 256, 16, 16,  884992      depth_4_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_5_bn (BatchNormalization) (None, 256, 16, 16,  1024        depth_5_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_5_relu (Activation)       (None, 256, 16, 16,  0           depth_5_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3D)  (None, 256, 8, 8, 4) 0           depth_5_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_6_conv (Conv3D)           (None, 256, 8, 8, 4) 1769728     max_pooling3d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depth_6_bn (BatchNormalization) (None, 256, 8, 8, 4) 1024        depth_6_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_6_relu (Activation)       (None, 256, 8, 8, 4) 0           depth_6_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "depth_7_conv (Conv3D)           (None, 512, 8, 8, 4) 3539456     depth_6_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_7_bn (BatchNormalization) (None, 512, 8, 8, 4) 2048        depth_7_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_7_relu (Activation)       (None, 512, 8, 8, 4) 0           depth_7_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3D)  (None, 512, 16, 16,  0           depth_7_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 768, 16, 16,  0           up_sampling3d_4[0][0]            \n",
      "                                                                 depth_5_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_8_conv (Conv3D)           (None, 256, 16, 16,  5308672     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_8_bn (BatchNormalization) (None, 256, 16, 16,  1024        depth_8_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_8_relu (Activation)       (None, 256, 16, 16,  0           depth_8_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "depth_9_conv (Conv3D)           (None, 256, 16, 16,  1769728     depth_8_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_9_bn (BatchNormalization) (None, 256, 16, 16,  1024        depth_9_conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_9_relu (Activation)       (None, 256, 16, 16,  0           depth_9_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_5 (UpSampling3D)  (None, 256, 32, 32,  0           depth_9_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384, 32, 32,  0           up_sampling3d_5[0][0]            \n",
      "                                                                 depth_3_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_10_conv (Conv3D)          (None, 128, 32, 32,  1327232     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_10_bn (BatchNormalization (None, 128, 32, 32,  512         depth_10_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_10_relu (Activation)      (None, 128, 32, 32,  0           depth_10_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "depth_11_conv (Conv3D)          (None, 128, 32, 32,  442496      depth_10_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_11_bn (BatchNormalization (None, 128, 32, 32,  512         depth_11_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_11_relu (Activation)      (None, 128, 32, 32,  0           depth_11_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_6 (UpSampling3D)  (None, 128, 64, 64,  0           depth_11_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192, 64, 64,  0           up_sampling3d_6[0][0]            \n",
      "                                                                 depth_1_relu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "depth_12_conv (Conv3D)          (None, 64, 64, 64, 3 331840      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_12_bn (BatchNormalization (None, 64, 64, 64, 3 256         depth_12_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_12_relu (Activation)      (None, 64, 64, 64, 3 0           depth_12_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "depth_13_conv (Conv3D)          (None, 64, 64, 64, 3 110656      depth_12_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_13_bn (BatchNormalization (None, 64, 64, 64, 3 256         depth_13_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "depth_13_relu (Activation)      (None, 64, 64, 64, 3 0           depth_13_bn[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 16,324,864\n",
      "Trainable params: 16,320,192\n",
      "Non-trainable params: 4,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model = unet_model_3d((1, 64, 64, 32), batch_normalization=True)\n",
    "keras_model.load_weights(keras_weight_path)\n",
    "output = keras_model.get_layer('depth_13_relu').output\n",
    "keras_model = keras.models.Model(inputs=keras_model.input, outputs=output)\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth_0_conv_1/kernel:0 (3, 3, 3, 1, 32)\n",
      "depth_0_conv_1/bias:0 (32,)\n",
      "depth_0_bn_1/gamma:0 (32,)\n",
      "depth_0_bn_1/beta:0 (32,)\n",
      "depth_0_bn_1/moving_mean:0 (32,)\n",
      "depth_0_bn_1/moving_variance:0 (32,)\n",
      "depth_1_conv_1/kernel:0 (3, 3, 3, 32, 64)\n",
      "depth_1_conv_1/bias:0 (64,)\n",
      "depth_1_bn_1/gamma:0 (64,)\n",
      "depth_1_bn_1/beta:0 (64,)\n",
      "depth_1_bn_1/moving_mean:0 (64,)\n",
      "depth_1_bn_1/moving_variance:0 (64,)\n",
      "depth_2_conv_1/kernel:0 (3, 3, 3, 64, 64)\n",
      "depth_2_conv_1/bias:0 (64,)\n",
      "depth_2_bn_1/gamma:0 (64,)\n",
      "depth_2_bn_1/beta:0 (64,)\n",
      "depth_2_bn_1/moving_mean:0 (64,)\n",
      "depth_2_bn_1/moving_variance:0 (64,)\n",
      "depth_3_conv_1/kernel:0 (3, 3, 3, 64, 128)\n",
      "depth_3_conv_1/bias:0 (128,)\n",
      "depth_3_bn_1/gamma:0 (128,)\n",
      "depth_3_bn_1/beta:0 (128,)\n",
      "depth_3_bn_1/moving_mean:0 (128,)\n",
      "depth_3_bn_1/moving_variance:0 (128,)\n",
      "depth_4_conv_1/kernel:0 (3, 3, 3, 128, 128)\n",
      "depth_4_conv_1/bias:0 (128,)\n",
      "depth_4_bn_1/gamma:0 (128,)\n",
      "depth_4_bn_1/beta:0 (128,)\n",
      "depth_4_bn_1/moving_mean:0 (128,)\n",
      "depth_4_bn_1/moving_variance:0 (128,)\n",
      "depth_5_conv_1/kernel:0 (3, 3, 3, 128, 256)\n",
      "depth_5_conv_1/bias:0 (256,)\n",
      "depth_5_bn_1/gamma:0 (256,)\n",
      "depth_5_bn_1/beta:0 (256,)\n",
      "depth_5_bn_1/moving_mean:0 (256,)\n",
      "depth_5_bn_1/moving_variance:0 (256,)\n",
      "depth_6_conv_1/kernel:0 (3, 3, 3, 256, 256)\n",
      "depth_6_conv_1/bias:0 (256,)\n",
      "depth_6_bn_1/gamma:0 (256,)\n",
      "depth_6_bn_1/beta:0 (256,)\n",
      "depth_6_bn_1/moving_mean:0 (256,)\n",
      "depth_6_bn_1/moving_variance:0 (256,)\n",
      "depth_7_conv_1/kernel:0 (3, 3, 3, 256, 512)\n",
      "depth_7_conv_1/bias:0 (512,)\n",
      "depth_7_bn_1/gamma:0 (512,)\n",
      "depth_7_bn_1/beta:0 (512,)\n",
      "depth_7_bn_1/moving_mean:0 (512,)\n",
      "depth_7_bn_1/moving_variance:0 (512,)\n",
      "depth_8_conv_1/kernel:0 (3, 3, 3, 768, 256)\n",
      "depth_8_conv_1/bias:0 (256,)\n",
      "depth_8_bn_1/gamma:0 (256,)\n",
      "depth_8_bn_1/beta:0 (256,)\n",
      "depth_8_bn_1/moving_mean:0 (256,)\n",
      "depth_8_bn_1/moving_variance:0 (256,)\n",
      "depth_9_conv_1/kernel:0 (3, 3, 3, 256, 256)\n",
      "depth_9_conv_1/bias:0 (256,)\n",
      "depth_9_bn_1/gamma:0 (256,)\n",
      "depth_9_bn_1/beta:0 (256,)\n",
      "depth_9_bn_1/moving_mean:0 (256,)\n",
      "depth_9_bn_1/moving_variance:0 (256,)\n",
      "depth_10_conv_1/kernel:0 (3, 3, 3, 384, 128)\n",
      "depth_10_conv_1/bias:0 (128,)\n",
      "depth_10_bn_1/gamma:0 (128,)\n",
      "depth_10_bn_1/beta:0 (128,)\n",
      "depth_10_bn_1/moving_mean:0 (128,)\n",
      "depth_10_bn_1/moving_variance:0 (128,)\n",
      "depth_11_conv_1/kernel:0 (3, 3, 3, 128, 128)\n",
      "depth_11_conv_1/bias:0 (128,)\n",
      "depth_11_bn_1/gamma:0 (128,)\n",
      "depth_11_bn_1/beta:0 (128,)\n",
      "depth_11_bn_1/moving_mean:0 (128,)\n",
      "depth_11_bn_1/moving_variance:0 (128,)\n",
      "depth_12_conv_1/kernel:0 (3, 3, 3, 192, 64)\n",
      "depth_12_conv_1/bias:0 (64,)\n",
      "depth_12_bn_1/gamma:0 (64,)\n",
      "depth_12_bn_1/beta:0 (64,)\n",
      "depth_12_bn_1/moving_mean:0 (64,)\n",
      "depth_12_bn_1/moving_variance:0 (64,)\n",
      "depth_13_conv_1/kernel:0 (3, 3, 3, 64, 64)\n",
      "depth_13_conv_1/bias:0 (64,)\n",
      "depth_13_bn_1/gamma:0 (64,)\n",
      "depth_13_bn_1/beta:0 (64,)\n",
      "depth_13_bn_1/moving_mean:0 (64,)\n",
      "depth_13_bn_1/moving_variance:0 (64,)\n"
     ]
    }
   ],
   "source": [
    "keras_layer_names = [weight.name for layer in keras_model.layers for weight in layer.weights]\n",
    "keras_weights = keras_model.get_weights()\n",
    "for name, weight in zip(keras_layer_names, keras_weights):\n",
    "    print(name, weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelsGenesisSegNet(\n",
      "  (in_block): _InBlock(\n",
      "    (conv1): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm1): BatchNorm3d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm2): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down_block1): _DownBlock(\n",
      "    (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm2): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down_block2): _DownBlock(\n",
      "    (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm1): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm2): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (down_block3): _DownBlock(\n",
      "    (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm1): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (conv2): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (norm2): BatchNorm3d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (up_block1): _UpBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (body): Sequential(\n",
      "      (conv1): Conv3d(768, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm1): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm2): BatchNorm3d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up_block2): _UpBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (body): Sequential(\n",
      "      (conv1): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm1): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm2): BatchNorm3d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (up_block3): _UpBlock(\n",
      "    (upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (body): Sequential(\n",
      "      (conv1): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (norm2): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (out_block): _OutBlock(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      ")\n",
      "Trainable parameters: 16.320257 M\n",
      "Memory usage: 62.256839752197266 MB\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = ModelsGenesisSegNet(in_channels=1, out_channels=1)\n",
    "pytorch_model.eval()\n",
    "pytorch_model.up_block3.body.relu2.register_forward_hook(forward_hook)\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "moving_idx = []\n",
    "for (pytorch_name, param) in pytorch_model.named_parameters():\n",
    "    try:\n",
    "        weight = keras_weights[idx]\n",
    "    except:\n",
    "        break\n",
    "    if 'conv' in pytorch_name and 'weight' in pytorch_name:\n",
    "        param.data = torch.from_numpy(np.transpose(weight, (4, 3, 2, 0, 1)))\n",
    "    else:\n",
    "        param.data = torch.from_numpy(weight)\n",
    "        \n",
    "    if 'norm' in pytorch_name and 'bias' in pytorch_name:\n",
    "        moving_idx.append(idx+1)\n",
    "        moving_idx.append(idx+2)\n",
    "        idx += 3\n",
    "    else:\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization running mean / var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_block.norm1 [0.10773456 0.0156241  0.07792253 1.1480795  0.02501738 0.09905564\n",
      " 0.19020303 0.41333827 0.08917138 0.6359106  0.3284189  2.2120872\n",
      " 0.15645984 0.14176716 0.14004321 0.12683997 0.06108763 0.32820657\n",
      " 0.05714194 0.04843716 0.4967732  0.10198742 0.10573964 0.08588026\n",
      " 0.6854433  0.06085574 0.0786974  0.294242   0.27014118 0.08691396\n",
      " 0.17866541 0.04669904]\n",
      "in_block.norm2 [ 3.7916975  3.8988621 19.649761   3.6805334  5.529005   2.2508006\n",
      "  4.4314017 10.00479    2.4139037  4.296064   2.122373   2.119363\n",
      "  6.2018237  9.0575905  2.5414429 14.059286   4.6595473  4.4932485\n",
      "  3.884869   5.5471125  4.308636   1.9267719  2.3988662  4.2054043\n",
      "  2.7728386  3.1002681  6.69639    3.6184905  4.2444334  2.0157583\n",
      "  2.1239355  3.3594143  2.9200778  2.024735   6.532754   2.6827655\n",
      "  3.7127178  2.5198367  3.071708   2.3560147  1.0687444  2.2905595\n",
      "  8.323762  12.777962   3.3162234  2.023244   2.1988385  3.094626\n",
      "  2.5355198  2.0601332  2.2036538  9.026318   2.1935406 20.833904\n",
      "  2.4403563  1.5032153  4.7400274  6.433666   2.6699774  5.3299646\n",
      "  3.1262183  1.3227553  2.1983764  3.3870492]\n",
      "down_block1.norm1 [ 6.1462054  7.6782184  6.389042   1.919737   3.850461   4.9641147\n",
      "  6.684048   4.2734275  4.5582395 13.952857   6.340304   6.1918473\n",
      "  5.404715   3.3949523  5.4418683  3.7115633  3.9437895  3.0004597\n",
      "  4.123242   5.801854   4.642024   8.155697   9.862208   3.9582942\n",
      "  8.830568   5.222882   8.861351   5.0622463  7.1855497  5.487201\n",
      "  6.5262947  5.6245923  5.1085587 11.097796   3.5182333  9.698608\n",
      " 16.592619  17.267012   7.065238   3.2060127  4.050174   5.515941\n",
      " 18.612812   6.0562825  5.334606  18.491766   4.3001256  2.4163969\n",
      "  3.7413073  4.398094   5.751623   6.5075927  3.2229602  9.802769\n",
      "  4.703571   6.5170245  3.6909049  5.618018   3.807053   5.268686\n",
      "  9.5009985 11.713405   6.319813   6.0119114]\n",
      "down_block1.norm2 [0.8559216  2.4047954  0.907138   0.84505874 1.2183455  1.2035344\n",
      " 0.5201757  1.2783536  1.5985644  1.9481637  1.128455   0.5536942\n",
      " 2.0873494  3.0629787  1.4230471  0.5310738  2.1736603  1.5979886\n",
      " 0.66878563 1.3177047  1.418351   0.89667654 2.6579695  0.76261675\n",
      " 1.0006888  0.6781313  1.2244141  1.5660926  1.5638584  1.6549568\n",
      " 1.1326295  2.166886   1.5163988  1.9049455  1.1498443  1.1432326\n",
      " 1.2913042  1.9010036  1.0460067  1.4743619  0.6629475  3.9451835\n",
      " 1.4950767  1.9271406  1.4241207  3.096569   1.1870259  1.3774854\n",
      " 3.3864355  0.93799543 1.6930938  1.7024968  0.8753665  1.3215904\n",
      " 2.182858   2.613139   1.0839806  5.54999    0.65133375 1.3077796\n",
      " 1.2337991  1.9811983  0.9469631  1.5998386  1.3026135  1.3462238\n",
      " 1.7816765  2.2657502  1.3729312  4.10135    1.0969142  1.2062795\n",
      " 1.2118536  1.3621416  2.0592399  1.1780051  1.127114   0.84163713\n",
      " 2.4829028  0.94603276 3.0852659  1.8218956  0.8079949  1.0660082\n",
      " 1.113369   1.6239198  1.3855586  1.263948   1.124034   1.3919959\n",
      " 1.7838202  1.1065902  0.8903959  1.356037   1.5220649  1.9688894\n",
      " 1.0494112  0.89530134 1.3224547  1.6726104  0.7993179  1.1749481\n",
      " 1.9102937  1.0249696  1.6309149  1.6537997  1.0965039  1.0220195\n",
      " 1.3790717  0.8820816  0.48196852 0.52735955 0.9527079  0.5372831\n",
      " 2.3880208  1.8710532  1.2464476  1.1567451  1.456436   0.7583395\n",
      " 1.2484397  2.8758912  1.4833682  1.2236496  1.861202   1.8522638\n",
      " 0.85513335 2.1401248 ]\n",
      "down_block2.norm1 [ 4.846362   9.598745   3.1538668  6.6559396  5.924398   6.440978\n",
      " 10.355911   8.926124   7.358903   9.930004   4.2616444  6.0311923\n",
      "  5.2006936  3.5855343 17.314405   3.423848   4.479336   4.855085\n",
      "  5.1169586 12.657757   5.2860937  5.3372836  6.112014  11.946994\n",
      " 10.955388   7.842758   6.336444   4.058907  15.778017   4.769228\n",
      "  7.3488464  9.782399  10.586774   7.1356726  6.822345   6.8035693\n",
      "  4.9760065  7.280858  10.748886  10.104632   6.0200644  4.47853\n",
      "  5.3898673 11.0654     4.0892515  7.7230425  9.767782   4.500733\n",
      " 12.16635    6.201251   6.4689927  5.354556   3.8380768  4.819579\n",
      "  9.180409   3.296689   5.2110157  4.7662272  6.8604174  6.9714994\n",
      "  8.464291   9.518748   5.714841   3.0558891  7.5903788  8.896164\n",
      "  5.681221   3.3406627 13.091026   4.424926   4.1596427  6.0194764\n",
      "  7.718924   4.1261544  7.8826737  4.649943  10.277106   5.323356\n",
      " 16.713596   6.7209387  4.8619013  7.590458   8.882076   5.785203\n",
      " 12.950837   6.458331   6.094762   4.037406   4.659619   9.00477\n",
      "  5.879932   5.795462   6.330877   7.025754   7.2242737  4.262936\n",
      "  5.7238927  3.9149604  8.283027   2.8964107  6.427102   5.0654674\n",
      "  7.424821   7.0888276  6.8490396  3.1925583  5.505531   9.544414\n",
      "  8.295135   4.8534274 11.014623   8.020279   5.158615   6.884231\n",
      "  9.092538   3.9807456 10.261969   8.201844   4.7848186  6.4113483\n",
      "  9.331172  10.619794   4.6855545  9.479282  10.219087   5.975754\n",
      "  5.9445696  6.498886 ]\n",
      "down_block2.norm2 [1.3861524  1.3313266  1.3175529  1.4003073  1.1687156  1.8338715\n",
      " 1.564321   0.9340475  0.9515983  1.1613641  1.1060295  1.5002971\n",
      " 1.277901   0.77367723 1.0850672  1.4170169  0.8142349  1.038938\n",
      " 1.5917846  3.1342971  0.93918395 0.96647066 1.2380176  1.763929\n",
      " 0.8445344  2.0338967  1.9928297  2.4601264  1.3287654  1.4308237\n",
      " 1.3095274  1.83705    1.2287376  1.9235275  1.6764505  0.94076234\n",
      " 0.6279297  2.680882   1.1902115  0.982745   2.994025   2.561287\n",
      " 1.4958438  0.7783709  1.0521911  1.5807482  0.9292881  1.8109056\n",
      " 0.9004086  1.3753326  0.9705485  1.5790052  0.86937255 1.1981741\n",
      " 1.0644705  1.2648078  1.1659636  2.8749352  1.0785887  1.4292148\n",
      " 1.0879053  1.384909   1.0902842  2.2256887  3.9657402  1.0659447\n",
      " 2.1082401  0.89938843 1.0965554  0.8730927  1.3694913  1.4176316\n",
      " 0.90228313 1.3609536  0.7909281  1.8102053  1.7258601  1.408361\n",
      " 0.846258   1.1755761  2.2297354  0.84991336 1.3889307  1.3029383\n",
      " 1.5793295  1.3389595  1.2806298  1.9762219  1.2849784  2.0855582\n",
      " 0.83394337 0.83896476 1.4577274  1.0493968  1.0290654  0.7845672\n",
      " 1.3426169  0.73839563 1.1193562  1.116684   1.566843   0.6310038\n",
      " 1.5725248  1.0453473  1.7952737  1.5840888  0.8271764  2.350301\n",
      " 1.2972832  1.1451173  1.7483882  1.4824281  2.0302393  1.688661\n",
      " 1.9044858  0.95356303 1.9113339  1.121311   1.0197691  0.74916035\n",
      " 1.3120896  1.8037124  0.94332755 1.1432827  0.8024591  1.3570405\n",
      " 2.3020413  1.0567546  0.7307963  0.7829501  1.5400364  1.4251262\n",
      " 1.2695427  1.8051403  0.97813153 1.5931079  0.87607294 1.238679\n",
      " 1.1997087  1.1025804  1.3120226  1.391343   6.361643   0.8232401\n",
      " 1.1221067  1.2521763  1.0155051  1.5560097  1.0828031  1.5558441\n",
      " 0.924494   1.1099573  1.754364   1.2865379  1.2467668  0.9560867\n",
      " 1.5216563  0.95314956 1.7202642  1.3097764  1.3931497  2.6759486\n",
      " 1.5203143  2.5699003  2.775786   1.3488741  0.8680491  2.3031752\n",
      " 0.9815671  0.877909   0.92262214 2.3073554  2.0083654  2.0037684\n",
      " 1.5784618  1.4030054  0.93706506 1.0531851  0.95819974 1.6484517\n",
      " 1.3202511  1.6710316  2.10273    1.282413   1.1156458  1.2406019\n",
      " 0.93435335 0.82511824 1.0007182  1.5729712  1.2519624  0.8672281\n",
      " 2.7100616  1.7124109  1.9793127  1.3989607  0.6543749  1.3269639\n",
      " 0.8353767  1.5107021  0.909575   1.6911179  2.215165   1.2071753\n",
      " 0.8037866  1.083962   1.1409534  1.0726738  1.7358541  0.8823658\n",
      " 1.134998   1.0307304  1.4265382  1.1167377  0.9948085  1.5099049\n",
      " 1.5754222  2.9396703  1.657507   1.3442334  1.0100178  0.7740093\n",
      " 1.1265587  1.2540592  2.5001876  1.5721984  1.1985621  1.9922019\n",
      " 2.0687323  1.3373868  1.8701264  1.5867097  1.8378341  1.2480797\n",
      " 0.93663365 1.0229815  1.474932   1.1216474  1.2657202  1.5133144\n",
      " 1.236554   1.4518003  1.8325294  1.3916975  1.6731089  1.3322548\n",
      " 0.86304873 0.94710004 1.0595293  1.0803351  1.5708759  0.86130244\n",
      " 2.885395   1.1645015  0.8093563  1.025827  ]\n",
      "down_block3.norm1 [6.7623916 3.4183824 4.8387246 3.9853542 4.5752945 6.07676   4.3609343\n",
      " 3.5152283 3.7014463 6.0648694 3.2590833 6.98883   3.8512888 5.8458986\n",
      " 3.840713  2.334904  3.5533712 3.875649  5.0763235 2.7037833 2.5314448\n",
      " 5.025396  4.6575227 3.0347996 2.1003375 2.8254697 5.472303  4.449656\n",
      " 3.5808144 7.8714485 5.052636  2.4154122 4.1791086 4.2235813 2.8236024\n",
      " 3.2862473 3.138896  2.855912  3.9836063 3.848037  2.7347953 2.988526\n",
      " 3.907367  3.8033082 2.9742973 2.3718514 2.3596804 2.767297  3.6839986\n",
      " 2.7901845 3.777803  3.516109  2.6680262 4.911775  1.9612749 3.6489356\n",
      " 6.515531  7.095368  3.5791965 4.963298  3.3572042 3.247397  3.1387515\n",
      " 2.7560947 6.5771294 2.459772  3.4274554 3.542122  2.3441482 3.2108407\n",
      " 2.9765625 6.3278527 6.107931  2.4144237 2.5283291 3.1995816 1.9347367\n",
      " 2.0466888 2.1704247 3.7717283 2.4381912 3.6647182 3.7965968 4.8190007\n",
      " 3.3316212 2.8618789 4.6139345 2.5686913 2.1662679 2.5680764 3.6451294\n",
      " 6.308682  4.0091343 2.86095   7.7046604 4.8532476 3.2184    4.5048814\n",
      " 2.5938823 4.1350126 3.3879247 3.5941052 2.9040692 5.4708686 9.691459\n",
      " 4.560361  5.083259  6.319321  2.9782488 2.0383358 3.6567757 1.985431\n",
      " 2.5825748 2.957093  3.6214948 4.764811  3.578148  3.1251464 3.0109122\n",
      " 4.203396  3.0055196 3.1428928 4.312474  2.1754084 2.694478  4.262729\n",
      " 3.8228142 3.144318  1.853545  4.678938  2.5441177 2.4660456 5.0529733\n",
      " 3.9858754 3.660444  2.5526133 2.2583308 2.0086813 3.6200755 3.7926378\n",
      " 4.448973  3.3295884 4.871246  4.5919394 5.467482  6.567392  4.437625\n",
      " 2.1402419 3.3570502 3.9768858 3.4120455 3.1333592 4.160203  1.9125907\n",
      " 6.417765  4.741863  4.665733  4.867893  2.6041937 2.7284951 3.4369678\n",
      " 2.27068   2.6798735 2.8945634 2.8625808 5.87461   3.4997985 6.5517197\n",
      " 3.3394527 3.2258918 3.8360233 4.2173767 4.956316  2.9355025 3.0869715\n",
      " 2.3874984 2.7161582 2.4220908 2.3602834 2.2837584 9.819312  2.1960435\n",
      " 4.425602  3.9305978 2.679602  3.430431  2.672546  2.5881019 2.9491212\n",
      " 2.6287856 2.8871691 2.3699095 3.576931  7.6218977 4.491842  2.1552742\n",
      " 4.5784082 3.9777536 2.8247647 3.7880392 2.4406848 3.288543  2.8479776\n",
      " 4.7239327 3.473796  1.956071  4.9237776 2.4972079 4.879554  3.5853949\n",
      " 2.7595503 2.977393  2.2678196 5.8699036 3.805892  2.3831353 3.5728517\n",
      " 3.0745919 3.648069  2.9037802 2.8114235 3.329368  4.124978  3.0427318\n",
      " 3.9460857 5.9239435 3.3571224 5.1426516 4.724391  4.8941507 2.686406\n",
      " 2.275251  3.5054643 5.1113815 1.8683718 6.452593  2.0330667 3.3285265\n",
      " 6.9798155 3.8229744 5.5622673 3.681432  2.2226136 2.9801736 7.386089\n",
      " 6.8738537 3.212504  2.6379943 2.7076695 3.7334833 6.2185707 4.380322\n",
      " 3.9529524 3.6727226 4.534348  2.611354 ]\n",
      "down_block3.norm2 [1.0661228  0.87808436 0.8989603  0.57099795 0.56509024 0.6727556\n",
      " 0.8777928  1.7612944  0.4740479  0.95266235 0.6361061  0.7877668\n",
      " 0.7370714  1.1657814  0.86037576 0.6177476  0.80724686 0.7181989\n",
      " 0.6801537  1.3693959  0.49998406 0.95164555 0.82170826 0.5859275\n",
      " 0.6995438  0.5155132  0.41681105 0.4161556  0.61547494 0.5406907\n",
      " 0.6351786  0.6577856  0.33746484 1.9056207  0.71461046 0.3951553\n",
      " 0.5626428  1.0651324  0.84862953 1.4187126  1.1608855  1.254011\n",
      " 0.49388877 0.54818815 0.68122506 0.87254614 1.4047214  0.38780883\n",
      " 0.71907485 0.88057864 0.47365212 0.6513325  1.1825035  0.6981581\n",
      " 0.67228645 0.5141007  0.7727209  0.6645375  1.3037117  1.1990901\n",
      " 0.3036231  1.0464036  0.76246494 1.616577   0.5175219  0.39920565\n",
      " 0.7360984  1.0272746  1.3811171  0.7092598  0.3265053  0.7786818\n",
      " 0.60428375 1.0768518  1.638044   0.5783654  0.40110844 2.3732977\n",
      " 0.35923904 0.9479041  0.534312   0.45772326 0.9422281  1.2235016\n",
      " 0.50174284 0.4305141  0.44519687 0.9048638  0.45657277 0.68594706\n",
      " 1.9838006  0.56118983 1.8776283  0.3988629  0.45570162 0.84501827\n",
      " 0.48382306 0.47376776 0.53623    0.342448   0.47509712 0.6809167\n",
      " 0.53722894 1.5133113  0.75547683 0.91896176 0.64574915 1.19949\n",
      " 1.181813   0.8665726  2.301181   0.86752707 0.6928048  0.52355605\n",
      " 2.1754103  1.1170884  0.7735686  0.74077415 1.1105489  1.2107198\n",
      " 1.1673799  1.3961513  0.6609754  1.1013887  0.54189575 0.49139458\n",
      " 1.0586579  0.31263152 0.56248164 0.67294097 0.78163254 0.7813259\n",
      " 0.42370892 0.46180153 1.0503186  0.78175384 0.7570261  0.48293555\n",
      " 0.9373923  0.5657846  0.9194002  0.6434889  0.6565214  0.54779494\n",
      " 0.3620903  0.5018479  0.56872797 1.0643121  0.60616636 0.92448306\n",
      " 0.3893324  1.2918277  0.68367356 0.84375095 0.706012   0.9276684\n",
      " 0.5990407  0.732805   0.9210356  0.40795937 0.77662444 0.44443685\n",
      " 0.9231432  2.6154222  0.69572514 0.42083508 0.41061106 0.64923584\n",
      " 0.77164567 1.0372369  0.6478655  0.43045536 0.55977523 1.2347083\n",
      " 1.372627   1.4206613  0.60445637 0.5684629  0.45785007 0.37124002\n",
      " 0.6562971  0.81433815 0.93367535 0.76925105 0.83082056 0.7342742\n",
      " 0.42774132 0.65859663 0.6363206  0.44432232 0.64563406 0.9110603\n",
      " 0.5658939  0.5299989  1.3341105  1.084404   0.30336627 1.1874659\n",
      " 0.7209022  0.27658135 1.7122912  0.7214528  0.42656267 0.98820806\n",
      " 0.4900765  0.515401   0.381287   0.55135596 0.7499346  1.1110235\n",
      " 0.6207527  1.5145127  1.2438033  1.6786262  0.49231404 0.64327484\n",
      " 0.36420277 0.4279174  2.7416196  0.55605686 2.006489   0.37112373\n",
      " 0.45542702 0.48953936 0.9942604  0.5569264  0.50877696 0.3215196\n",
      " 0.4919876  1.2345235  0.41050926 0.53095955 0.7408035  1.695333\n",
      " 1.2977984  0.5072284  2.4945414  0.55183583 0.49208635 0.6806559\n",
      " 0.6606172  0.6768057  0.8293153  1.543622   0.6100848  1.0825621\n",
      " 1.7953748  0.5401727  1.1011834  1.2521473  0.68731296 0.54466105\n",
      " 0.5413753  0.59027857 0.44001514 1.1785899  0.67742723 0.98675466\n",
      " 1.218124   0.53528696 0.3941327  0.6460133  1.0997559  0.83065885\n",
      " 0.41854513 0.6051712  0.79087013 1.0917637  0.61927    0.40643665\n",
      " 0.28768936 0.52765155 0.40644172 0.6419043  0.3523185  0.7123615\n",
      " 0.7633506  0.9180769  1.5346433  2.209515   0.6849177  0.9452437\n",
      " 0.8914763  0.9537818  0.84786433 0.47129604 0.6818523  0.68376714\n",
      " 0.9433831  0.42956743 0.69276524 0.36860713 1.2271858  0.756787\n",
      " 0.6095639  0.9061605  0.46202204 0.7713272  1.1249006  0.53885597\n",
      " 1.1163588  1.3122431  0.7137301  0.95245606 1.1189327  0.9496251\n",
      " 0.733973   0.51711744 0.9131615  0.55906826 0.66354215 0.42627916\n",
      " 0.892706   0.47939125 0.46645796 1.1138487  0.72245884 1.386697\n",
      " 0.72843945 0.9179227  0.6445411  0.98759484 0.906242   1.0641164\n",
      " 0.9340033  1.6520153  0.62805355 0.32145655 1.4457458  0.39101455\n",
      " 1.0625621  0.3505561  0.749938   0.9194298  1.6223699  1.8264675\n",
      " 0.7865446  0.65482104 0.75276417 0.54901326 0.72845256 0.6659704\n",
      " 0.68866277 0.38656992 0.51469415 0.69288725 0.45808885 0.35203713\n",
      " 0.6251944  0.616795   0.75393414 0.7899703  1.254015   0.69618535\n",
      " 0.5879693  0.9432881  0.47946677 0.46211174 0.6876649  0.94854516\n",
      " 0.55757236 0.4531996  0.6637857  0.7418238  1.2324926  0.3546222\n",
      " 0.6231747  0.5096826  0.81451285 0.49020642 0.9125967  0.63329154\n",
      " 0.6201643  0.6945091  0.5670154  0.6284991  1.0945046  0.67592484\n",
      " 0.543778   0.593717   0.6208484  0.9745784  0.45837015 0.33456695\n",
      " 0.98134446 0.48690662 0.7555763  0.61753726 0.63104695 1.1235293\n",
      " 1.4268957  0.33654013 0.7494967  0.5878222  0.56697625 0.8051739\n",
      " 0.88301986 0.4276212  0.59922695 0.9341148  0.9312012  0.40786514\n",
      " 0.8047598  0.85714495 0.39456877 1.1304971  0.5547537  0.6687629\n",
      " 0.9460212  0.62655115 0.48183227 1.6226598  0.76696664 0.5210279\n",
      " 0.55857587 0.5978987  0.5836997  0.7515885  0.575212   0.55414844\n",
      " 0.8055904  0.4919275  0.94329584 1.211027   0.6551112  0.41406998\n",
      " 0.9007029  0.94888425 0.4170128  0.4427535  0.65524876 0.45888656\n",
      " 0.8206596  0.7644444  0.34906214 0.43572253 0.7533142  0.9128228\n",
      " 1.306736   0.9321347  0.42629302 1.2454287  0.46579072 1.1409677\n",
      " 0.87551475 0.8079346  0.5011543  1.3833879  1.0188843  0.5289966\n",
      " 0.8848144  0.72547585 1.571124   0.66203696 0.6037783  0.39839303\n",
      " 0.8775555  0.506398   1.0608605  0.8779141  0.38094258 0.8323503\n",
      " 0.46264657 0.4984822  1.0873916  1.1597024  0.63861215 0.41142172\n",
      " 0.7006933  0.62319505 0.52359104 0.6417284  0.4167522  0.88681716\n",
      " 0.77231586 0.68044555 1.1617967  0.48865512 0.77709246 0.5176244\n",
      " 0.74723345 0.59703684 0.43648404 0.8551577  1.0278292  0.76974624\n",
      " 0.96641314 0.40455076 0.45284763 0.7317546  0.51596963 0.58544445\n",
      " 0.46980122 0.9781713  0.59938544 0.5812762  0.87227565 0.8190449\n",
      " 0.66667926 0.8342936  1.0368016  0.48147574 0.59175646 0.40019846\n",
      " 0.48833492 0.74933386 0.5458875  0.6154227  1.3061686  0.43819386\n",
      " 0.84919906 0.4029804 ]\n",
      "up_block1.body.norm1 [ 3.3257487 18.974707   2.3263328  2.72309    2.9983203  5.579111\n",
      "  4.3617554  4.337601   3.0501707  1.5530099  2.0299182  6.96\n",
      "  3.5699108  4.84608    7.513339   7.1888175  6.0030556  9.214335\n",
      "  6.2693467  7.1471157  1.9098259  6.3782225  2.109892   3.187492\n",
      "  4.2506337  4.249123   5.9490504  2.898781   2.196462   3.9808054\n",
      "  5.939263   2.4737337  2.130307   6.4211717  3.6006572  1.9038855\n",
      "  2.9125097  5.426733  12.959759   1.5976455  1.8835529  6.6422863\n",
      "  6.9536757  6.4655237  3.710376   2.54535    5.7034793  5.899208\n",
      "  4.184261   2.635044  10.212429   3.7460678  3.8027442  6.196527\n",
      "  4.170523  31.909698   4.734035   5.1098347  7.4039545 18.942598\n",
      " 10.3646     6.3296676  2.9884567 12.081592   5.5952234  2.2456744\n",
      "  2.851343   3.6510594  1.9656223  5.002965   5.6137342  5.6700974\n",
      "  8.480782   2.6053855  2.3648334  4.2211885  2.3149428  3.0454886\n",
      "  5.9045386  9.836265   3.334512   9.847488   3.1295052  5.89105\n",
      "  3.8073156  4.3995543  1.7529578 10.017834  12.123825   2.516584\n",
      "  3.4936857  3.4587739 13.275383   1.4679604  1.2467399  2.5558188\n",
      "  4.435102   1.8002977  3.0863616  3.9596772  9.718236   5.5860825\n",
      "  3.4229083  4.2584705 36.99387    3.9982061  6.8909492  2.7182717\n",
      "  3.2423334 10.5436945 15.830619   4.3202124  4.6953807  1.8940619\n",
      "  9.615956   8.562411   2.8919537  5.5201244  3.8174248  2.5975587\n",
      "  3.3135808 10.63471    1.9749755  3.793663   2.4341178 10.1168785\n",
      " 12.834878   7.3094387  5.582736   4.176783   4.046624   8.902254\n",
      "  1.3829938  2.3751223 10.355495  11.551758   8.580613   8.783228\n",
      "  4.1254334  2.0237968  2.859201   2.4976923  9.58626    1.6704668\n",
      "  3.1017818  4.583667   7.75972    3.4208498  5.8458214  5.08975\n",
      "  3.4878104  1.9082756 12.933913   3.0478344  2.4598782  3.56213\n",
      "  4.730699   6.381579   8.430344   9.9742365  6.5871625  7.121158\n",
      "  2.501632  16.596241  13.4339     3.1061356  5.7612433  1.531403\n",
      "  4.3436136  5.4689107 12.517262   3.3435364  2.9053257  3.4432442\n",
      "  5.917654   2.8646252  2.5617645  5.2422357  4.2654314  1.5087544\n",
      "  5.020027   1.9182695  3.869323  12.473527   6.975172   3.211842\n",
      "  3.9224985  2.4172857  8.483446   6.392295   4.0618234 13.111455\n",
      " 11.574581   4.418782   3.9922264  5.4635735  2.2662225  3.81062\n",
      "  1.8793623  5.877064   2.2642717  6.370609   3.812665   1.6472459\n",
      "  5.1168838  3.106709   2.0587761  2.3123057  2.1794803  8.051866\n",
      "  5.056241   7.791355   1.5704825  8.187608   4.895705   2.0027678\n",
      "  7.093871   8.925537   5.389426   1.3878087  1.9828037  6.319231\n",
      "  9.222988   4.141739   2.8949206  3.4237945  1.8199196  3.259651\n",
      "  6.496186   7.917035   7.077778   2.801148   3.8334608  3.5398262\n",
      "  3.5596933  5.889651   4.829756   2.3741372  2.5454874  4.7075286\n",
      "  4.4149227  6.384737   3.6430311  3.6380975  6.706862   2.3345976\n",
      "  5.669479   3.7890296  4.1079     8.830143   4.764545   2.857341\n",
      "  3.4686491  2.146537   9.876416   1.7374765]\n",
      "up_block1.body.norm2 [ 1.6644799   5.3875117   1.1012203   1.0666319   1.7925893   1.2047929\n",
      "  4.3490353   3.2142127   2.9679258   7.9020276   5.7157054   1.6984203\n",
      "  6.1678047   6.168322    1.7047265   2.1270325   5.460802    3.930067\n",
      "  1.7273552   0.74584043  3.6363754   2.7259302   0.49344796  5.4448485\n",
      "  2.5838952   6.0727177   5.3642626   2.4195504   3.5148282   1.9312878\n",
      "  4.103043    5.4079046   5.4988475   0.9912457   6.624679    1.7180328\n",
      "  2.3728087   1.7423733   2.3853447   1.3890889   1.3619876   1.5934565\n",
      "  2.1697967   2.386157    3.1140568   2.974383    4.300101    1.2563231\n",
      "  6.0968547   1.6612109   1.5237215   3.1646533   3.7005413   1.1240574\n",
      "  2.8684862   9.133848    1.1128862   2.0640302   3.0885158   7.988457\n",
      "  1.2046511   3.5664146   2.511366    1.4859309   1.4945805   2.8881972\n",
      "  1.8008143   7.697172    2.3364651   0.977305    1.8521569   2.3460734\n",
      "  5.1717525   1.7967737   2.0159328   0.9286673   2.675953    1.8025143\n",
      "  3.2132757   7.1607194   0.7931664   3.2951212   1.9211034   2.2387173\n",
      "  0.80216694  2.4723046   1.1517183   3.3223336   3.339651    2.1062448\n",
      "  3.2387726   1.3980386   1.1757857   1.6205963   2.560464    2.9652293\n",
      "  7.655386    2.9283519   5.0404563   4.1041226   2.337472    0.8549868\n",
      "  2.292552    1.75054     3.3854103   0.96561813  2.861984    3.3958266\n",
      "  1.3565946   2.4777594   2.4710839   2.5692434   1.7848678   7.3561506\n",
      "  1.0376728   2.3170867   8.6105795   1.3124517  10.916521    1.3352014\n",
      "  3.1541767   3.1712003   1.306393    1.51226     3.308809    2.1978197\n",
      "  2.9580822   1.4942579   3.3126216   6.1012006   0.4900161   2.3660386\n",
      "  7.2396536   5.9491053   3.5122976   0.94951403  4.317254    4.5198274\n",
      "  1.533725    3.837242    2.3415685   5.378219    1.9720868   2.5158973\n",
      "  2.6211839   0.93691844  1.4410421   1.2817898   1.5647151   3.6903212\n",
      "  3.8994823   1.6515146   1.4529437   2.9107523   1.4643316   2.4006073\n",
      "  6.32117     1.0891716   1.8947253   7.6637793   2.073337    5.105417\n",
      "  1.5794653   1.2009633   3.1933787   1.9819204   0.532664    4.5500307\n",
      "  3.2114763   5.5333066   2.031152    1.4326414   5.0925355   1.3920072\n",
      "  1.488984    2.4777641   7.125176    5.556072    1.9646188   4.651078\n",
      "  0.7273477  13.616747    1.8087058   2.8445559   1.8602118   2.050281\n",
      "  3.1478357   6.6593804   4.143371    3.0660095   0.87361765  1.9509584\n",
      "  1.547739    2.6771953   5.4716005   1.3144873   4.741133    5.6301184\n",
      "  0.77875036  1.9319469   1.8013036   3.4208438   1.5587659   3.0784354\n",
      " 12.555617    1.5192077   7.4692354   8.433004    1.0253866   1.9731752\n",
      "  1.605198    1.1075544   4.1464825   3.6005716   2.2521112   0.9495365\n",
      "  3.6859722   5.338075    1.706052    1.1321257   4.173036    1.0506625\n",
      "  1.7465509   1.2847068   8.1378765   3.1134176   2.3598292   1.8839344\n",
      "  3.1506743   3.7025483   1.1704912   7.045247    2.623101    3.9026034\n",
      "  1.0959283   5.2528973   3.0360293   3.5799983   0.56511     7.0059714\n",
      "  3.4876785   0.6500879   2.1446996   2.0955915   2.7307723  12.525652\n",
      "  3.1781297   2.05982     3.7374015   1.21907     0.78806657  2.6022713\n",
      "  1.9461976   4.739824    1.1938949   1.9556764 ]\n",
      "up_block2.body.norm1 [10.043321   5.391291  11.307355   5.0695577  3.2110012  7.212299\n",
      "  4.6037903 13.357883   2.1308672  3.6215262 19.047113   9.359577\n",
      " 10.515944   3.6606007  9.533483  11.413236   2.7177708 10.019441\n",
      "  6.3381715 13.787706   4.89179    3.7535207  5.3557634  3.0660498\n",
      "  3.9834971 10.431395  10.546875   5.3250704  6.1866107 14.654113\n",
      " 11.891786   5.98964   13.291469   5.2800393  7.7396483  4.0802917\n",
      "  5.583451   5.1380606  3.900464   5.491073   4.5790586 15.891713\n",
      "  4.0665975 10.75861    4.2886934  1.9331743  7.154761  10.776788\n",
      " 39.097248  12.411343   2.2550178 50.096294  32.54631   14.114178\n",
      "  8.700326  24.722132   4.513334   4.4437194  9.678409  23.448284\n",
      " 13.9835005  6.7054257  1.7613097 10.076404   4.5190682  2.4910939\n",
      " 15.780758   7.2106013  3.996519   7.2217827 13.751151   3.742697\n",
      "  6.3729463  3.542026   1.7130203  3.332229   6.2387414  6.739849\n",
      "  9.361976   4.0863013  3.9267209 22.353474  14.9287815  3.822699\n",
      " 15.903786  14.363467   6.871477   2.9471347  5.5291243  5.5291686\n",
      "  6.362763  10.523771   8.083153  20.887089   2.5027008  9.328362\n",
      "  9.456121  10.129082  21.57903   13.275743   5.0833344 12.106832\n",
      "  8.120131   7.589706   5.9727077  4.0929317 15.186926   5.725722\n",
      " 10.6802635  6.98666   20.95561   11.488668  11.1688385  7.59072\n",
      " 19.668468   9.425348   8.1961    10.787673  35.852695  12.7627125\n",
      " 10.336814  29.666466  25.410173  10.159927  15.153232   6.0477514\n",
      "  4.3998346  5.042111 ]\n",
      "up_block2.body.norm2 [ 2.3680186  4.6816773  3.6367316  5.33878    4.6211457 48.405243\n",
      " 17.699806   3.402805   1.8640465  4.7265673  5.3050036  8.473942\n",
      "  4.498397   5.440682   4.4170356  1.0828444 12.356164   3.192594\n",
      "  4.458611   6.328498   7.713028   2.4743314  1.5653886  4.016677\n",
      "  2.5690753  3.883954   6.800777   8.925107  10.48422    8.597598\n",
      " 10.508872   6.8072515  6.410963   2.822323   3.7285736 13.901652\n",
      " 22.497828   2.0757504  3.781455  12.009973  11.245037  19.78888\n",
      "  3.5701845  9.616852   2.4657154  7.5502014 12.102979   3.5000134\n",
      " 12.649077   3.4214342  6.0606933  3.8585606  7.9282837  3.0929897\n",
      "  3.9396522  6.81876   13.912618   3.9477823  5.906525   3.9603326\n",
      " 16.226883   3.982863  27.976719  10.013432   4.231875   3.2530582\n",
      "  1.404158   6.1993923 11.755721   5.051496  21.238075   2.2614582\n",
      "  9.376554  19.584856   5.376249   9.998574   1.4211568  2.9391484\n",
      "  7.107734   1.8152976  9.288636   2.696298   4.9919367  5.2418213\n",
      "  2.5572534 11.923748   3.4563878  4.9245243  4.296365  14.5269785\n",
      "  3.109057   4.192333   5.068457   6.436528   4.1635613  3.4883652\n",
      "  3.8537357 18.549835   5.7704206  3.7708848 14.94351    5.8408914\n",
      " 13.121665  13.6179905  4.473105   5.471348   2.5240264  3.198823\n",
      " 14.533722   5.315697   5.825695   1.9916205 12.063605   5.8263264\n",
      " 12.047599   5.5097203  2.6851206  1.4160442 30.585323   8.433403\n",
      " 15.1317835  6.6750617  3.1271873  9.050158   6.2748055  4.011684\n",
      "  7.311389   4.3093643]\n",
      "up_block3.body.norm1 [  2.6928284    7.344081     3.852285    12.603141    44.945778\n",
      "  31.148209     3.7801049   70.49672     63.297554    18.776848\n",
      "  22.080545    10.050371     0.6038459    7.077735    43.4344\n",
      "  13.841424    13.146721    14.559606    33.728546     8.113033\n",
      "   8.807271     6.824195     6.7001657   22.063246    12.022064\n",
      "  37.65929     45.108784     9.847318   155.90839     13.920094\n",
      "  10.042442    90.66744     38.472694     0.56205845  12.015714\n",
      "  17.64372     58.49395     53.309135    12.312604    46.688698\n",
      "  12.932606    53.782223    13.776238     2.7355876   47.771282\n",
      "  23.893436     2.7336867  120.373726    11.304488     3.7934825\n",
      "  60.075825     3.057948    12.225001    21.35308     17.005821\n",
      "  22.688124     9.251954    50.363194     9.097459     4.930699\n",
      "  23.373741    13.761917   145.39699     26.981176  ]\n",
      "up_block3.body.norm2 [  6.318113  420.4341      7.0462294  14.836899    3.7745264  38.80265\n",
      "  11.334229   16.549967    4.0367017   2.292718    3.712521   14.610113\n",
      "   4.656316    6.9663706   6.2626133  21.46002    27.377851   10.890915\n",
      "   3.7776413   3.2392871   6.988439    5.783963   13.702088    3.8768795\n",
      "   3.6160996   2.6037917   1.2181952  23.652952    3.2164392   7.6032553\n",
      "  10.552388    3.100634    3.3894427  29.082596   33.35615     4.674\n",
      "   8.973054    9.16088     4.5392246   5.772216    2.6375904   3.0298436\n",
      "  41.5238      2.903049    3.842214    3.313267  427.60526     8.198011\n",
      "   3.2354019   6.008889   12.7529545  16.821383   31.053202    4.366085\n",
      "  17.854311    3.5220647   9.055429   14.393888   14.109783   18.717215\n",
      "  58.580635    9.166334    4.0520954   3.3632836]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for name, module in pytorch_model.named_modules():\n",
    "    if 'norm' in name:\n",
    "        print(name, keras_weights[moving_idx[idx+1]])\n",
    "        module.running_mean = torch.from_numpy(keras_weights[moving_idx[idx]])\n",
    "        module.running_var = torch.from_numpy(keras_weights[moving_idx[idx+1]])\n",
    "        idx += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate whether the transformation is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hook = []\n",
    "input = np.random.randn(1, 1, 64, 64, 32).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_input = torch.from_numpy(input.transpose(0, 1, 4, 2, 3)).float().to(torch.device('cuda:0'))\n",
    "pytorch_model.to(torch.device('cuda:0'))\n",
    "pytorch_model(pytorch_input)\n",
    "pytorch_output = features_hook[0].detach().cpu().numpy().transpose(0, 1, 3, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_output = keras_model.predict_on_batch(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005493164\n"
     ]
    }
   ],
   "source": [
    "assert keras_output.shape == pytorch_output.shape, f\"{keras_output.shape} != {pytorch_output.shape}\"\n",
    "print(np.max(np.abs(keras_output - pytorch_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = pytorch_model.state_dict()\n",
    "state_dict.pop('out_block.weight')\n",
    "state_dict.pop('out_block.bias')\n",
    "torch.save(state_dict, pytorch_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the segmentation / classification network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import keras\n",
    "import torch\n",
    "from ModelsGenesis.keras.unet3d import *\n",
    "from src.model import ModelsGenesisSegNet, ModelsGenesisClfNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_weight_path = './ModelsGenesis/pretrained_weights/Genesis_Chest_CT.h5' # ModelGenesis pretrained weights\n",
    "pytorch_weight_path = './weights/models_genesis.pth' # path to save the transformed Pytorch weights\n",
    "\n",
    "def forward_hook(self, inputs, outputs):\n",
    "    features_hook.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hook = []\n",
    "input = np.random.randn(1, 1, 64, 64, 32).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f9a702278d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_net = ModelsGenesisSegNet(in_channels=1, out_channels=10, weight_path=pytorch_weight_path)\n",
    "seg_net.eval()\n",
    "seg_net.up_block3.body.relu2.register_forward_hook(forward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_input = torch.from_numpy(input.transpose(0, 1, 4, 2, 3)).float().to(torch.device('cuda:0'))\n",
    "seg_net.to(torch.device('cuda:0'))\n",
    "seg_net(pytorch_input)\n",
    "pytorch_output = features_hook[0].detach().cpu().numpy().transpose(0, 1, 3, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = unet_model_3d((1, 64, 64, 32), batch_normalization=True)\n",
    "keras_model.load_weights(keras_weight_path)\n",
    "output = keras_model.get_layer('depth_13_relu').output\n",
    "keras_model = keras.models.Model(inputs=keras_model.input, outputs=output)\n",
    "keras_output = keras_model.predict_on_batch(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000579834"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(pytorch_output - keras_output).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hook = []\n",
    "input = np.random.randn(1, 1, 64, 64, 32).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f9a70267e48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_net = ModelsGenesisClfNet(in_channels=1, out_channels=20, weight_path=pytorch_weight_path)\n",
    "clf_net.eval()\n",
    "clf_net.down_block3.relu2.register_forward_hook(forward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_input = torch.from_numpy(input.transpose(0, 1, 4, 2, 3)).float().to(torch.device('cuda:0'))\n",
    "clf_net.to(torch.device('cuda:0'))\n",
    "clf_net(pytorch_input)\n",
    "pytorch_output = features_hook[0].detach().cpu().numpy().transpose(0, 1, 3, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = unet_model_3d((1, 64, 64, 32), batch_normalization=True)\n",
    "keras_model.load_weights(keras_weight_path)\n",
    "output = keras_model.get_layer('depth_7_relu').output\n",
    "keras_model = keras.models.Model(inputs=keras_model.input, outputs=output)\n",
    "keras_output = keras_model.predict_on_batch(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.670288e-05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(pytorch_output - keras_output).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
